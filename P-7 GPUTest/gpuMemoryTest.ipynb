{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e71a5a6",
   "metadata": {},
   "source": [
    "---\n",
    "# GPU Memory Stress Test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de007cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c83420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA is available!\n",
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9b384",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e555f",
   "metadata": {},
   "source": [
    "### üîç Step-by-Step Explanation\n",
    "\n",
    "- `torch.randn(10000, 10000, device=device)` Creates two large random matrices `a` and `b` of size `10,000 √ó 10,000` directly on the specified device (cpu or cuda).\n",
    "\n",
    "- `torch.matmul(a, b)` Performs matrix multiplication between `a` and `b`. This is a computationally heavy operation, ideal for benchmarking.\n",
    "\n",
    "- `start = time.time()` and `end = time.time()` Measures the time taken to perform the operation.\n",
    "\n",
    "- `torch.cuda.synchronize()` Ensures all GPU operations finish before recording the end time. GPU operations are asynchronous by default, so without this, the timing might be inaccurate. It's skipped for CPU.\n",
    "\n",
    "- `print(...)` Displays the device used and the time taken for the operation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5e3c7",
   "metadata": {},
   "source": [
    "`a = 1000*1000 Matrix` and `b = 1000*1000 Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e458af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running a test tensor operation...\n",
      "‚úÖ Operation completed on: cuda\n",
      "‚è± Time taken: 1.394 seconds\n"
     ]
    }
   ],
   "source": [
    "# Perform a small matrix operation to test GPU\n",
    "print(\"\\nRunning a test tensor operation...\")\n",
    "\n",
    "start = time.time()\n",
    "a = torch.randn(1000, 1000, device=device)\n",
    "b = torch.randn(1000, 1000, device=device)\n",
    "c = torch.matmul(a, b)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None  # Ensure all ops finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚úÖ Operation completed on: {device}\")\n",
    "print(f\"‚è± Time taken: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52738f37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316e835",
   "metadata": {},
   "source": [
    "`a = 10000*10000 Matrix` and `b = 10000*10000 Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cf9ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running a test tensor operation...\n",
      "‚úÖ Operation completed on: cuda\n",
      "‚è± Time taken: 2.659 seconds\n"
     ]
    }
   ],
   "source": [
    "# Perform a small matrix operation to test GPU\n",
    "print(\"\\nRunning a test tensor operation...\")\n",
    "\n",
    "start = time.time()\n",
    "a = torch.randn(10000, 10000, device=device)\n",
    "b = torch.randn(10000, 10000, device=device)\n",
    "c = torch.matmul(a, b)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None  # Ensure all ops finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚úÖ Operation completed on: {device}\")\n",
    "print(f\"‚è± Time taken: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e884e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()   # Frees unused GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26bf36",
   "metadata": {},
   "source": [
    "---\n",
    "## Second Test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c68de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA is available!\n",
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0308fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_duration = 180  # seconds\n",
    "matrix_size = 10000    # increase for heavier load (e.g., 8192)\n",
    "log_interval = 10     # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13aeb942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Warming up GPU...\n",
      "\n",
      "üöÄ Starting sustained load test...\n",
      "‚è± 11.2s elapsed | Iterations: 5 | Memory Allocated: 1.61 GB | Reserved: 1.63 GB\n",
      "‚è± 20.0s elapsed | Iterations: 9 | Memory Allocated: 1.71 GB | Reserved: 1.72 GB\n",
      "‚è± 31.4s elapsed | Iterations: 14 | Memory Allocated: 1.83 GB | Reserved: 1.85 GB\n",
      "‚è± 40.2s elapsed | Iterations: 17 | Memory Allocated: 1.91 GB | Reserved: 1.93 GB\n",
      "‚è± 52.7s elapsed | Iterations: 21 | Memory Allocated: 2.02 GB | Reserved: 2.03 GB\n",
      "‚è± 60.3s elapsed | Iterations: 24 | Memory Allocated: 2.10 GB | Reserved: 2.12 GB\n",
      "‚è± 71.3s elapsed | Iterations: 28 | Memory Allocated: 2.22 GB | Reserved: 2.23 GB\n",
      "‚è± 83.7s elapsed | Iterations: 31 | Memory Allocated: 2.30 GB | Reserved: 2.31 GB\n",
      "‚è± 92.0s elapsed | Iterations: 33 | Memory Allocated: 2.36 GB | Reserved: 2.37 GB\n",
      "‚è± 103.5s elapsed | Iterations: 36 | Memory Allocated: 2.45 GB | Reserved: 2.46 GB\n",
      "‚è± 111.8s elapsed | Iterations: 38 | Memory Allocated: 2.51 GB | Reserved: 2.53 GB\n",
      "‚è± 123.0s elapsed | Iterations: 41 | Memory Allocated: 2.60 GB | Reserved: 2.62 GB\n",
      "‚è± 135.7s elapsed | Iterations: 42 | Memory Allocated: 2.64 GB | Reserved: 2.65 GB\n",
      "‚è± 141.5s elapsed | Iterations: 43 | Memory Allocated: 2.67 GB | Reserved: 2.68 GB\n",
      "‚è± 163.1s elapsed | Iterations: 44 | Memory Allocated: 2.70 GB | Reserved: 2.71 GB\n",
      "‚è± 167.1s elapsed | Iterations: 45 | Memory Allocated: 2.73 GB | Reserved: 2.75 GB\n",
      "‚è± 254.8s elapsed | Iterations: 46 | Memory Allocated: 2.77 GB | Reserved: 2.78 GB\n",
      "\n",
      "‚úÖ Test completed in 254.80 seconds\n",
      "üîÅ Total iterations completed: 46\n"
     ]
    }
   ],
   "source": [
    "# Warm-up\n",
    "print(\"\\n‚è≥ Warming up GPU...\")\n",
    "a = torch.randn(matrix_size, matrix_size, device=device)\n",
    "b = torch.randn(matrix_size, matrix_size, device=device)\n",
    "_ = torch.matmul(a, b)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"\\nüöÄ Starting sustained load test...\")\n",
    "start_time = time.time()\n",
    "next_log = start_time + log_interval\n",
    "iterations = 0\n",
    "\n",
    "while True:\n",
    "    matrix_size += 400\n",
    "    a = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    b = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    iterations += 1\n",
    "    torch.cuda.empty_cache()   # Frees unused GPU memory\n",
    "    current_time = time.time()\n",
    "    if current_time >= next_log:\n",
    "        elapsed = current_time - start_time\n",
    "        mem_allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        mem_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "        print(f\"‚è± {elapsed:.1f}s elapsed | Iterations: {iterations} | \"\n",
    "              f\"Memory Allocated: {mem_allocated:.2f} GB | Reserved: {mem_reserved:.2f} GB\")\n",
    "        next_log += log_interval\n",
    "\n",
    "    if current_time - start_time >= target_duration:\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Test completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"üîÅ Total iterations completed: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07a2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
