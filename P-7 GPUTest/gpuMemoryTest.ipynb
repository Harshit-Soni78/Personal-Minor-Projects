{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e71a5a6",
   "metadata": {},
   "source": [
    "---\n",
    "# GPU Memory Stress Test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de007cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c83420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA is available!\n",
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9b384",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e555f",
   "metadata": {},
   "source": [
    "### üîç Step-by-Step Explanation\n",
    "\n",
    "- `torch.randn(10000, 10000, device=device)` Creates two large random matrices `a` and `b` of size `10,000 √ó 10,000` directly on the specified device (cpu or cuda).\n",
    "\n",
    "- `torch.matmul(a, b)` Performs matrix multiplication between `a` and `b`. This is a computationally heavy operation, ideal for benchmarking.\n",
    "\n",
    "- `start = time.time()` and `end = time.time()` Measures the time taken to perform the operation.\n",
    "\n",
    "- `torch.cuda.synchronize()` Ensures all GPU operations finish before recording the end time. GPU operations are asynchronous by default, so without this, the timing might be inaccurate. It's skipped for CPU.\n",
    "\n",
    "- `print(...)` Displays the device used and the time taken for the operation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5e3c7",
   "metadata": {},
   "source": [
    "`a = 1000*1000 Matrix` and `b = 1000*1000 Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e458af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running a test tensor operation...\n",
      "‚úÖ Operation completed on: cuda\n",
      "‚è± Time taken: 1.394 seconds\n"
     ]
    }
   ],
   "source": [
    "# Perform a small matrix operation to test GPU\n",
    "print(\"\\nRunning a test tensor operation...\")\n",
    "\n",
    "start = time.time()\n",
    "a = torch.randn(1000, 1000, device=device)\n",
    "b = torch.randn(1000, 1000, device=device)\n",
    "c = torch.matmul(a, b)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None  # Ensure all ops finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚úÖ Operation completed on: {device}\")\n",
    "print(f\"‚è± Time taken: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52738f37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316e835",
   "metadata": {},
   "source": [
    "`a = 10000*10000 Matrix` and `b = 10000*10000 Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cf9ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running a test tensor operation...\n",
      "‚úÖ Operation completed on: cuda\n",
      "‚è± Time taken: 2.659 seconds\n"
     ]
    }
   ],
   "source": [
    "# Perform a small matrix operation to test GPU\n",
    "print(\"\\nRunning a test tensor operation...\")\n",
    "\n",
    "start = time.time()\n",
    "a = torch.randn(10000, 10000, device=device)\n",
    "b = torch.randn(10000, 10000, device=device)\n",
    "c = torch.matmul(a, b)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None  # Ensure all ops finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚úÖ Operation completed on: {device}\")\n",
    "print(f\"‚è± Time taken: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e884e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc38fc",
   "metadata": {},
   "source": [
    "`a = 100000*100000 Matrix` and `b = 100000*100000 Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7f31bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running a test tensor operation...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 37.25 GiB. GPU 0 has a total capacity of 4.00 GiB of which 2.07 GiB is free. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 11.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning a test tensor operation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m a = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m b = torch.randn(\u001b[32m100000\u001b[39m, \u001b[32m100000\u001b[39m, device=device)\n\u001b[32m      7\u001b[39m c = torch.matmul(a, b)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 37.25 GiB. GPU 0 has a total capacity of 4.00 GiB of which 2.07 GiB is free. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 11.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Perform a small matrix operation to test GPU\n",
    "print(\"\\nRunning a test tensor operation...\")\n",
    "\n",
    "start = time.time()\n",
    "a = torch.randn(100000, 100000, device=device)\n",
    "b = torch.randn(100000, 100000, device=device)\n",
    "c = torch.matmul(a, b)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None  # Ensure all ops finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚úÖ Operation completed on: {device}\")\n",
    "print(f\"‚è± Time taken: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6a531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
